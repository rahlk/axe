Rules of Timm:
 _____  _                        _       
/__   \(_) _ __ ___   _ __ ___  ( ) ___  
  / /\/| || '_ ` _ \ | '_ ` _ \ |/ / __| 
 / /   | || | | | | || | | | | |   \__ \ 
 \/    |_||_| |_| |_||_| |_| |_|   |___/ 
                                         
              _                          
 _ __  _   _ | |  ___  ___   _           
| '__|| | | || | / _ \/ __| (_)          
| |   | |_| || ||  __/\__ \  _           
|_|    \__,_||_| \___||___/ (_)   


2.7: Two point seven
Use Python 2.7+, not Python 3, since many useful libraries are
NOT yet ported to Python 3.
 
ANYTIME: 
For long computations, implement occasional "dump to disk"
and "restart from part-way". That way, if the long comptuation
crashes, you can restart from some interim point (and
not redo it all).

ATMC: Add the missing code.
All python files should start with 
from __future__ import division
import sys
sys.dont_write_bytecode = True

BBB: Burn baby burn
If it works, benchmark it against a simpler option.
Throw away what does not improve performance. 

BCD: Beware container defaults 
Default params to functions, methods are evaluated at
load time. Which means that N calls to a function with
an argument that defaults to, say, an empty list will
always be talking to the *same* list, everytime you
call the function. 

C: COMMITMENT:
Do it. Many times a day.

CA: CONSTANTS AREN'T:
Put the code in a function where
the 'constants' are defaults to function arguments (so
later, you can call it another way).

ESM: Effect size matters.
Statistical significance tests can condone very small
variations in large comptuations. So always use an effect
size test with the significance test to avoid small
number bullsh*t.

FES: Five equals six
Minor numeric differences in performance are usually
unstable and disappear when you run the experiment
again. So don't fret the small deltas. 
Always seek the "big *ss" differences.

FSSS: Fast stats, slow stats.
Two kinds of statistcal tests: fast and slow.
Fast tests (parametric, e.g. t-tests) are used as heuristics
during a run to check for (say) early stopping. Slow tests
(non-parametric, e.g. bootstraps) are used after the run
to confirm some experimental hypothesis.

IDM: It doesn't matter.
Don't waste time arguing in theory about some tuning
issue. Just try it out. It probably won't matter in
practice.

JDI: JUST DO IT: 
Systems are not written; they grow. So find the
smallest next thing and just do that. Repeat.

KISS: 
keep it simple stupid.

KTS: Keep the seed.
To allow for reproduction, keep the seed and print it
as part of the options.

LIB: LEAVE IT BROKEN:
At the end of the day, leave behind a broken test
(this is where you can start tomorrow).

NBO: NO BURIED OPTIONS:
Keep all 'The' settings. Print
'The' settings in front of all output.

NoGo: GLOBALS ARE EVIL:
N-1 globals is better than N globals.

NO W: NO WRAPS:
All code <= 52 characters. Not "self"
but "i". Indents using 2 spaces.

R: REFACTOR:
To code it once, just do it.
If you code it twice, wince. 
But if you code it thrice, refactor.

RNC: RANDOM NOT CRAZY 
Often the stochastic version is simpler, or scales
better, or both.

SO: SHOW OFF:
Code should know how to show off. All files end in
if __name__ == '__main__': doSomethingCool();

STF: TRUTH IS SHORTER THAN FICTION:
If I do not understand it, I can't code it succinctly.
So to check if I understand it, try to code it.

TAG: THINGS ARE GOOD
Consider not defining a new class if a simple Thing will do
(useful for named access to data only classes).

TDD: TEST DRIVEN DEVELOPMENT:
Write a test.
Write the code.
Fix the fault.
Run all tests
Fix the faults.
Repeat.

TTS: TEACH THE SOURCE:
Teach from the code. One lecture = what you can show
students in one hour.

YAGNI: 
You aren't gonna need it. Only code stuff
that is needed for your latest test. All other
generalizations are hallucinations.

YCIYR: YOUR CODE IS YOUR RESUME
When going for jobs, imagine sending prosepctive
employees your Github repo containing impressive
code. Think how much more interested that would
make them.
